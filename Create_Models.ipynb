{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import pygame\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.vec_env import VecFrameStack\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_atari_env\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env= gym.make(\"FrozenLake-v1\", desc=None, map_name = \"4x4\", is_slippery=False, render_mode = \"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "game 1, reward?: 0.0 \n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", desc=None, map_name = \"4x4\", is_slippery=False, render_mode = \"human\")\n",
    "pygame.init()\n",
    "observation, info = env.reset(seed=42)\n",
    "i = 1\n",
    "score = 0\n",
    "n = 20\n",
    "for _ in range(n):\n",
    "   env.render()\n",
    "   action = env.action_space.sample()  # this is where you would insert your policy\n",
    "   observation, reward, terminated, truncated, info = env.step(action)\n",
    "   score += reward\n",
    "   \n",
    "   if terminated or truncated:\n",
    "      print(\"game {}, reward?: {} \".format(i, score))\n",
    "      observation, info = env.reset()\n",
    "      i +=1 \n",
    "      \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", desc=None, map_name = \"4x4\", is_slippery=False)\n",
    "env= DummyVecEnv([lambda: env])\n",
    "model = PPO('MlpPolicy', env, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 908  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 2    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 647       |\n",
      "|    iterations           | 2         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 4096      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00382  |\n",
      "|    explained_variance   | 1         |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -4.52e-05 |\n",
      "|    n_updates            | 310       |\n",
      "|    policy_gradient_loss | -2.32e-05 |\n",
      "|    value_loss           | 6.09e-16  |\n",
      "---------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 602          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002981581 |\n",
      "|    clip_fraction        | 0.000439     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.00237     |\n",
      "|    explained_variance   | 1            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.000112    |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | -0.000792    |\n",
      "|    value_loss           | 1.48e-07     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 566          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 14           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003593808 |\n",
      "|    clip_fraction        | 0.000439     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.00223     |\n",
      "|    explained_variance   | 0.135        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.000145     |\n",
      "|    n_updates            | 330          |\n",
      "|    policy_gradient_loss | -0.000687    |\n",
      "|    value_loss           | 0.00205      |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 553           |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 18            |\n",
      "|    total_timesteps      | 10240         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.2118911e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0023       |\n",
      "|    explained_variance   | 0.521         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | -6.26e-05     |\n",
      "|    n_updates            | 340           |\n",
      "|    policy_gradient_loss | -3.19e-05     |\n",
      "|    value_loss           | 6.56e-06      |\n",
      "-------------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 535       |\n",
      "|    iterations           | 6         |\n",
      "|    time_elapsed         | 22        |\n",
      "|    total_timesteps      | 12288     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.3072471 |\n",
      "|    clip_fraction        | 0.104     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0297   |\n",
      "|    explained_variance   | 0.998     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0179   |\n",
      "|    n_updates            | 350       |\n",
      "|    policy_gradient_loss | -0.021    |\n",
      "|    value_loss           | 2.09e-08  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 527       |\n",
      "|    iterations           | 7         |\n",
      "|    time_elapsed         | 27        |\n",
      "|    total_timesteps      | 14336     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0936897 |\n",
      "|    clip_fraction        | 0.329     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.428    |\n",
      "|    explained_variance   | 0.0416    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0603   |\n",
      "|    n_updates            | 360       |\n",
      "|    policy_gradient_loss | -0.0355   |\n",
      "|    value_loss           | 0.00218   |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 527         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 31          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019689452 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.278      |\n",
      "|    explained_variance   | 0.147       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0438     |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.047      |\n",
      "|    value_loss           | 0.00101     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 524         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 35          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017395975 |\n",
      "|    clip_fraction        | 0.349       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.265      |\n",
      "|    explained_variance   | 0.343       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0541     |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0531     |\n",
      "|    value_loss           | 0.000445    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 522         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 39          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008917699 |\n",
      "|    clip_fraction        | 0.288       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.202      |\n",
      "|    explained_variance   | 0.606       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.058      |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.0462     |\n",
      "|    value_loss           | 0.000211    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 519        |\n",
      "|    iterations           | 11         |\n",
      "|    time_elapsed         | 43         |\n",
      "|    total_timesteps      | 22528      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06934557 |\n",
      "|    clip_fraction        | 0.254      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.106     |\n",
      "|    explained_variance   | 0.73       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0514    |\n",
      "|    n_updates            | 400        |\n",
      "|    policy_gradient_loss | -0.0437    |\n",
      "|    value_loss           | 0.000119   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 514         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 47          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.085126355 |\n",
      "|    clip_fraction        | 0.0282      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0112     |\n",
      "|    explained_variance   | 0.943       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0311     |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.0244     |\n",
      "|    value_loss           | 1.6e-05     |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 508           |\n",
      "|    iterations           | 13            |\n",
      "|    time_elapsed         | 52            |\n",
      "|    total_timesteps      | 26624         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00021586617 |\n",
      "|    clip_fraction        | 0.000781      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.00345      |\n",
      "|    explained_variance   | 0.996         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.000155      |\n",
      "|    n_updates            | 420           |\n",
      "|    policy_gradient_loss | -0.001        |\n",
      "|    value_loss           | 5.44e-07      |\n",
      "-------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 507        |\n",
      "|    iterations           | 14         |\n",
      "|    time_elapsed         | 56         |\n",
      "|    total_timesteps      | 28672      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05495185 |\n",
      "|    clip_fraction        | 0.0218     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0227    |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0352    |\n",
      "|    n_updates            | 430        |\n",
      "|    policy_gradient_loss | -0.00652   |\n",
      "|    value_loss           | 2.58e-09   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 506         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 60          |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015085783 |\n",
      "|    clip_fraction        | 0.306       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.237      |\n",
      "|    explained_variance   | 0.565       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0534     |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0495     |\n",
      "|    value_loss           | 0.000235    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 503          |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 65           |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062570674 |\n",
      "|    clip_fraction        | 0.216        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.186       |\n",
      "|    explained_variance   | 0.64         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0523      |\n",
      "|    n_updates            | 450          |\n",
      "|    policy_gradient_loss | -0.0433      |\n",
      "|    value_loss           | 0.00016      |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 502        |\n",
      "|    iterations           | 17         |\n",
      "|    time_elapsed         | 69         |\n",
      "|    total_timesteps      | 34816      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06641343 |\n",
      "|    clip_fraction        | 0.241      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0892    |\n",
      "|    explained_variance   | 0.802      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0322    |\n",
      "|    n_updates            | 460        |\n",
      "|    policy_gradient_loss | -0.0388    |\n",
      "|    value_loss           | 7.63e-05   |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 503        |\n",
      "|    iterations           | 18         |\n",
      "|    time_elapsed         | 73         |\n",
      "|    total_timesteps      | 36864      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06526719 |\n",
      "|    clip_fraction        | 0.0193     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.00748   |\n",
      "|    explained_variance   | 0.941      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0339    |\n",
      "|    n_updates            | 470        |\n",
      "|    policy_gradient_loss | -0.027     |\n",
      "|    value_loss           | 1.47e-05   |\n",
      "----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 502           |\n",
      "|    iterations           | 19            |\n",
      "|    time_elapsed         | 77            |\n",
      "|    total_timesteps      | 38912         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.9546437e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.00212      |\n",
      "|    explained_variance   | 0.999         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | -0.000284     |\n",
      "|    n_updates            | 480           |\n",
      "|    policy_gradient_loss | -0.000123     |\n",
      "|    value_loss           | 1.12e-07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 501           |\n",
      "|    iterations           | 20            |\n",
      "|    time_elapsed         | 81            |\n",
      "|    total_timesteps      | 40960         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.2265747e-05 |\n",
      "|    clip_fraction        | 0.000391      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0015       |\n",
      "|    explained_variance   | 0.999         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.68e-05      |\n",
      "|    n_updates            | 490           |\n",
      "|    policy_gradient_loss | -0.00059      |\n",
      "|    value_loss           | 2.57e-07      |\n",
      "-------------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 501       |\n",
      "|    iterations           | 21        |\n",
      "|    time_elapsed         | 85        |\n",
      "|    total_timesteps      | 43008     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.00148  |\n",
      "|    explained_variance   | 1         |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -5.07e-05 |\n",
      "|    n_updates            | 500       |\n",
      "|    policy_gradient_loss | -1.86e-05 |\n",
      "|    value_loss           | 3.58e-10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 501       |\n",
      "|    iterations           | 22        |\n",
      "|    time_elapsed         | 89        |\n",
      "|    total_timesteps      | 45056     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0014   |\n",
      "|    explained_variance   | 1         |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -5.25e-05 |\n",
      "|    n_updates            | 510       |\n",
      "|    policy_gradient_loss | -2.54e-05 |\n",
      "|    value_loss           | 1.24e-13  |\n",
      "---------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 502           |\n",
      "|    iterations           | 23            |\n",
      "|    time_elapsed         | 93            |\n",
      "|    total_timesteps      | 47104         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00021541104 |\n",
      "|    clip_fraction        | 0.000439      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.00142      |\n",
      "|    explained_variance   | 0.135         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 9.35e-06      |\n",
      "|    n_updates            | 520           |\n",
      "|    policy_gradient_loss | -0.000697     |\n",
      "|    value_loss           | 0.00206       |\n",
      "-------------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 504       |\n",
      "|    iterations           | 24        |\n",
      "|    time_elapsed         | 97        |\n",
      "|    total_timesteps      | 49152     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.0018   |\n",
      "|    explained_variance   | 0.709     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -1.14e-05 |\n",
      "|    n_updates            | 530       |\n",
      "|    policy_gradient_loss | -6.48e-06 |\n",
      "|    value_loss           | 3.68e-06  |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 505        |\n",
      "|    iterations           | 25         |\n",
      "|    time_elapsed         | 101        |\n",
      "|    total_timesteps      | 51200      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16570202 |\n",
      "|    clip_fraction        | 0.0522     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0299    |\n",
      "|    explained_variance   | 0.998      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0369    |\n",
      "|    n_updates            | 540        |\n",
      "|    policy_gradient_loss | -0.014     |\n",
      "|    value_loss           | 2.65e-08   |\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=50000)\n",
    "model.save(\"FrozenLake_model_testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m model \u001b[39m=\u001b[39m PPO\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39mFrozenLake_model_testing\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstable_baselines3\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcommon\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mevaluation\u001b[39;00m \u001b[39mimport\u001b[39;00m evaluate_policy\n\u001b[1;32m---> 12\u001b[0m evaluate_policy(model, env, n_eval_episodes\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, render\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\stable_baselines3\\common\\evaluation.py:94\u001b[0m, in \u001b[0;36mevaluate_policy\u001b[1;34m(model, env, n_eval_episodes, deterministic, render, callback, reward_threshold, return_episode_rewards, warn)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[39mwhile\u001b[39;00m (episode_counts \u001b[39m<\u001b[39m episode_count_targets)\u001b[39m.\u001b[39many():\n\u001b[0;32m     88\u001b[0m     actions, states \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(\n\u001b[0;32m     89\u001b[0m         observations,  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m     90\u001b[0m         state\u001b[39m=\u001b[39mstates,\n\u001b[0;32m     91\u001b[0m         episode_start\u001b[39m=\u001b[39mepisode_starts,\n\u001b[0;32m     92\u001b[0m         deterministic\u001b[39m=\u001b[39mdeterministic,\n\u001b[0;32m     93\u001b[0m     )\n\u001b[1;32m---> 94\u001b[0m     new_observations, rewards, dones, infos \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(actions)\n\u001b[0;32m     95\u001b[0m     current_rewards \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m rewards\n\u001b[0;32m     96\u001b[0m     current_lengths \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:197\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[39mStep the environments with the given action\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \n\u001b[0;32m    193\u001b[0m \u001b[39m:param actions: the action\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \u001b[39m:return: observation, reward, done, information\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_async(actions)\n\u001b[1;32m--> 197\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep_wait()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_wait\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m     56\u001b[0m     \u001b[39m# Avoid circular imports\u001b[39;00m\n\u001b[0;32m     57\u001b[0m     \u001b[39mfor\u001b[39;00m env_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_envs):\n\u001b[1;32m---> 58\u001b[0m         obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menvs[env_idx]\u001b[39m.\u001b[39;49mstep(\n\u001b[0;32m     59\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactions[env_idx]\n\u001b[0;32m     60\u001b[0m         )\n\u001b[0;32m     61\u001b[0m         \u001b[39m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[0;32m     62\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx] \u001b[39m=\u001b[39m terminated \u001b[39mor\u001b[39;00m truncated\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\shimmy\\openai_gym_compatibility.py:123\u001b[0m, in \u001b[0;36mGymV26CompatibilityV0.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action: ActType) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mtuple\u001b[39m[ObsType, \u001b[39mfloat\u001b[39m, \u001b[39mbool\u001b[39m, \u001b[39mbool\u001b[39m, \u001b[39mdict\u001b[39m]:\n\u001b[0;32m    115\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Steps through the environment.\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \n\u001b[0;32m    117\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[39m        (observation, reward, terminated, truncated, info)\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgym_env\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gym\\wrappers\\time_limit.py:50\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m     40\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \n\u001b[0;32m     42\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m \n\u001b[0;32m     49\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m     observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     51\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     53\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gym\\wrappers\\order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[0;32m     36\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling env.reset()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gym\\wrappers\\env_checker.py:39\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[39mreturn\u001b[39;00m env_step_passive_checker(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv, action)\n\u001b[0;32m     38\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 39\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gym\\envs\\toy_text\\frozen_lake.py:252\u001b[0m, in \u001b[0;36mFrozenLakeEnv.step\u001b[1;34m(self, a)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlastaction \u001b[39m=\u001b[39m a\n\u001b[0;32m    251\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 252\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender()\n\u001b[0;32m    253\u001b[0m \u001b[39mreturn\u001b[39;00m (\u001b[39mint\u001b[39m(s), r, t, \u001b[39mFalse\u001b[39;00m, {\u001b[39m\"\u001b[39m\u001b[39mprob\u001b[39m\u001b[39m\"\u001b[39m: p})\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gym\\envs\\toy_text\\frozen_lake.py:279\u001b[0m, in \u001b[0;36mFrozenLakeEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    277\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_render_text()\n\u001b[0;32m    278\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# self.render_mode in {\"human\", \"rgb_array\"}:\u001b[39;00m\n\u001b[1;32m--> 279\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_render_gui(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender_mode)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gym\\envs\\toy_text\\frozen_lake.py:373\u001b[0m, in \u001b[0;36mFrozenLakeEnv._render_gui\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    371\u001b[0m     pygame\u001b[39m.\u001b[39mevent\u001b[39m.\u001b[39mpump()\n\u001b[0;32m    372\u001b[0m     pygame\u001b[39m.\u001b[39mdisplay\u001b[39m.\u001b[39mupdate()\n\u001b[1;32m--> 373\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclock\u001b[39m.\u001b[39;49mtick(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmetadata[\u001b[39m\"\u001b[39;49m\u001b[39mrender_fps\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[0;32m    374\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrgb_array\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    375\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mtranspose(\n\u001b[0;32m    376\u001b[0m         np\u001b[39m.\u001b[39marray(pygame\u001b[39m.\u001b[39msurfarray\u001b[39m.\u001b[39mpixels3d(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwindow_surface)), axes\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[0;32m    377\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Create the Gym environment\n",
    "env = gym.make(\"FrozenLake-v1\", desc=None, map_name=\"4x4\", is_slippery=False, render_mode=\"human\")\n",
    "\n",
    "# Wrap the environment in DummyVecEnv to set the random seed\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env.seed(42)  # Set the random seed for reproducibility\n",
    "\n",
    "# Load the pre-trained PPO model\n",
    "model = PPO.load(\"FrozenLake_model_testing\")\n",
    "\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "evaluate_policy(model, env, n_eval_episodes=10, render=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
